{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01da3d9f",
   "metadata": {},
   "source": [
    "# Dense Object Detection on SKU-110K (RetinaNet & LocalRetinaNet)\n",
    "\n",
    "This notebook contains a **complete training & evaluation pipeline** for:\n",
    "\n",
    "- A **torchvision RetinaNet baseline** (`retinanet_resnet50_fpn`)\n",
    "- A custom **LocalRetinaNet** model with:\n",
    "  - ResNet-50 + FPN backbone (P3–P5)\n",
    "  - Multi-level anchors\n",
    "  - Local correlation-aware focal loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa08d1",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "5f196675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T21:19:17.967271Z",
     "start_time": "2025-12-10T21:19:17.953956Z"
    }
   },
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as Fnn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpycocotools\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcoco\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m COCO\n\u001B[32m     25\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpycocotools\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcocoeval\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m COCOeval\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mgoogle\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcolab\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[32m     29\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mPyTorch:\u001B[39m\u001B[33m\"\u001B[39m, torch.__version__)\n\u001B[32m     30\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTorchvision:\u001B[39m\u001B[33m\"\u001B[39m, torchvision.__version__)\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'google'"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "a0f96d21",
   "metadata": {},
   "source": [
    "## 2. Download and Mount Pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b108ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_DIR = \"./output/pt-models\"\n",
    "# Example: https://drive.google.com/file/d/FILE_ID/view?usp=sharing\n",
    "PRETRAINED_FILE_ID = \"1M8TJoZ-P8wiU1KLGlSDpRUggNavzCWia\"\n",
    "ZIP_PATH = \"./pt-models.zip\"\n",
    "\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "os.makedirs(PRETRAINED_DIR, exist_ok=True)\n",
    "\n",
    "existing_files = [f for f in os.listdir(PRETRAINED_DIR) if f.endswith(\".pth\")]\n",
    "\n",
    "if len(existing_files) > 0:\n",
    "    print(\"Pretrained models already available:\")\n",
    "    for f in existing_files:\n",
    "        print(\" •\", f)\n",
    "    print(\"\\nSkipping download...\")\n",
    "else:\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    print(\"Downloading pretrained models from Google Drive...\")\n",
    "    !gdown --id $PRETRAINED_FILE_ID -O $ZIP_PATH\n",
    "\n",
    "    print(\"Extracting ZIP...\")\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        for member in zip_ref.namelist():\n",
    "            # Skip macOS metadata\n",
    "            if member.startswith('__MACOSX') or member.endswith('.DS_Store'):\n",
    "                continue\n",
    "\n",
    "            # Extract into output/pt-models while flattening structure\n",
    "            filename = os.path.basename(member)\n",
    "            if not filename:\n",
    "                continue  # skip folders\n",
    "\n",
    "            source = zip_ref.open(member)\n",
    "            target_path = os.path.join(PRETRAINED_DIR, filename)\n",
    "\n",
    "            with open(target_path, \"wb\") as target:\n",
    "                with source as src:\n",
    "                    target.write(src.read())\n",
    "\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        os.remove(ZIP_PATH)\n",
    "        print(f\"Removed ZIP file: {ZIP_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2236a",
   "metadata": {},
   "source": [
    "## 3. Download and Mount Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34228bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data/SKU110K_modified\"\n",
    "ZIP_PATH = \"./SKU110K_modified.zip\"\n",
    "# Example: https://drive.google.com/file/d/FILE_ID/view?usp=sharing\n",
    "FILE_ID = \"1QrZ6zTbOSiE28TQkBExb4Fa7EM6i5mfr\"\n",
    "\n",
    "# If folder already exists, skip download\n",
    "if os.path.exists(DATA_DIR):\n",
    "    print(\"Dataset already exists — skipping download.\")\n",
    "else:\n",
    "    os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    print(\"Downloading dataset ZIP...\")\n",
    "    !gdown --id $FILE_ID -O $ZIP_PATH\n",
    "\n",
    "    print(\"Extracting ZIP...\")\n",
    "\n",
    "    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
    "        for member in zip_ref.infolist():\n",
    "\n",
    "            # Skip macOS junk files\n",
    "            if member.filename.startswith(\"__MACOSX\") or member.filename.endswith(\".DS_Store\"):\n",
    "                continue\n",
    "\n",
    "            zip_ref.extract(member, \"./data\")\n",
    "\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "    # Remove ZIP file\n",
    "    if os.path.exists(ZIP_PATH):\n",
    "        os.remove(ZIP_PATH)\n",
    "        print(f\"Removed ZIP file: {ZIP_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25606526",
   "metadata": {},
   "source": [
    "## 4. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "471a6387",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:11:59.078340Z",
     "start_time": "2025-12-10T06:11:59.075630Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# UTILS\n",
    "# ======================================================================================\n",
    "\n",
    "def timestamp():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "def ensure_dir(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def fix_coco_json(path):\n",
    "    \"\"\"\n",
    "    Add missing fields so pycocotools does not crash on annotation JSON.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Prediction JSON is a list -> ignore\n",
    "    if isinstance(data, list):\n",
    "        return\n",
    "\n",
    "    changed = False\n",
    "\n",
    "    if \"info\" not in data:\n",
    "        data[\"info\"] = {}\n",
    "        changed = True\n",
    "\n",
    "    if \"licenses\" not in data:\n",
    "        data[\"licenses\"] = []\n",
    "        changed = True\n",
    "\n",
    "    if changed:\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "\n",
    "def pad_images_to_batch(images):\n",
    "    \"\"\"\n",
    "    Pad list of CHW images to max height and max width within the batch.\n",
    "    Keeps content in top-left, pads bottom/right with zeros.\n",
    "    \"\"\"\n",
    "    import torch.nn.functional as F_pad\n",
    "\n",
    "    max_h = max(img.shape[1] for img in images)\n",
    "    max_w = max(img.shape[2] for img in images)\n",
    "\n",
    "    padded = []\n",
    "    for img in images:\n",
    "        _, h, w = img.shape\n",
    "        pad_bottom = max_h - h\n",
    "        pad_right = max_w - w\n",
    "        img_padded = F_pad.pad(img, (0, pad_right, 0, pad_bottom))\n",
    "        padded.append(img_padded)\n",
    "\n",
    "    return torch.stack(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051561d0",
   "metadata": {},
   "source": [
    "## 5. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c641cad5713c4aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# CONFIGURATION\n",
    "# ======================================================================================\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Dataset paths\n",
    "        self.train_images = \"./data/SKU110K_modified/images\"\n",
    "        self.val_images = \"./data/SKU110K_modified/images\"\n",
    "        self.test_images = \"./data/SKU110K_modified/images\"\n",
    "\n",
    "        self.train_annotations = \"./data/SKU110K_modified/annotations/COCO_json/annotations_train.json\"\n",
    "        self.val_annotations = \"./data/SKU110K_modified/annotations/COCO_json/annotations_val.json\"\n",
    "        self.test_annotations = \"./data/SKU110K_modified/annotations/COCO_json/annotations_test.json\"\n",
    "\n",
    "        # Inference\n",
    "        self.infer_image_path = \"./data/SKU110K_modified/images/test_0.jpg\"\n",
    "\n",
    "        # Local RetinaNet (correlation-aware) hyperparameters\n",
    "        self.num_classes_local = 1        # single \"product\" class\n",
    "        self.lambda_reg_local = 0.1       # weight between cls and reg for local model\n",
    "        self.num_epochs_local = 5\n",
    "        self.batch_size_local = 1         # memory-safe for LocalRetinaNet\n",
    "\n",
    "        # Baseline RetinaNet hyperparameters\n",
    "        self.num_classes_retina = 2       # torchvision RetinaNet classes (background + product)\n",
    "        self.num_epochs_retina = 5\n",
    "        self.batch_size_retina = 1\n",
    "\n",
    "        self.lr = 1e-4\n",
    "        self.num_workers = 0\n",
    "\n",
    "        # Save paths\n",
    "        self.save_local_model_path = \"./output/pt-models/retinanet_local_sku110k.pth\"\n",
    "        self.save_retinanet_model_path = \"./output/pt-models/retinanet_sku110k.pth\"\n",
    "\n",
    "        self.device = torch.device(\n",
    "            \"mps\" if torch.backends.mps.is_available()\n",
    "            else \"cuda\" if torch.cuda.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "\n",
    "\n",
    "config = Config()\n",
    "print(\"Using device:\", config.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a6a9e9",
   "metadata": {},
   "source": [
    "## 6. Dataset & transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129b316a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:11:59.089103Z",
     "start_time": "2025-12-10T06:11:59.085210Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# DATASET (COCO FORMAT)\n",
    "# ======================================================================================\n",
    "\n",
    "class SKU110K_COCO(Dataset):\n",
    "    def __init__(self, root, annotation_json, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "        with open(annotation_json, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        self.images = {img[\"id\"]: img for img in data[\"images\"]}\n",
    "        self.ids = sorted(self.images.keys())\n",
    "\n",
    "        self.annos = {img_id: [] for img_id in self.ids}\n",
    "        for ann in data[\"annotations\"]:\n",
    "            self.annos[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        info = self.images[img_id]\n",
    "\n",
    "        fname = info[\"file_name\"].split(\"/\")[-1]\n",
    "        img_path = os.path.join(self.root, fname)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in self.annos[img_id]:\n",
    "            x, y, w, h = ann[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(1)  # single product class (id=1)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor(int(img_id)),\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "        else:\n",
    "            img = F.to_tensor(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "class ToTensorOnly:\n",
    "    \"\"\"Transform that converts PIL to tensor and leaves boxes unchanged.\"\"\"\n",
    "    def __call__(self, img, target):\n",
    "        img = F.to_tensor(img)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "class ResizeForDetection:\n",
    "    def __init__(self, max_side=1024, stride=32):\n",
    "        self.max_side = max_side\n",
    "        self.stride = stride  # ensure divisibility for FPN\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        # img is PIL\n",
    "        w, h = img.size\n",
    "        scale = self.max_side / max(h, w)\n",
    "        if scale < 1.0:\n",
    "            new_w = int(w * scale)\n",
    "            new_h = int(h * scale)\n",
    "\n",
    "            # Make divisible by FPN stride\n",
    "            new_w = (new_w // self.stride) * self.stride\n",
    "            new_h = (new_h // self.stride) * self.stride\n",
    "\n",
    "            img = img.resize((new_w, new_h))\n",
    "\n",
    "            # Resize boxes too:\n",
    "            boxes = target[\"boxes\"]\n",
    "            boxes = boxes * scale\n",
    "            target[\"boxes\"] = boxes\n",
    "\n",
    "        img = F.to_tensor(img)\n",
    "        return img, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b418c3",
   "metadata": {},
   "source": [
    "## 7. Torchvision RetinaNet baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf0705d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:11:59.093703Z",
     "start_time": "2025-12-10T06:11:59.091315Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# BASELINE RETINANET (torchvision)\n",
    "# ======================================================================================\n",
    "\n",
    "def create_retinanet(num_classes):\n",
    "    model = torchvision.models.detection.retinanet_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    num_anchors = model.head.classification_head.num_anchors\n",
    "\n",
    "    # replace cls head to match num_classes\n",
    "    model.head.classification_head.num_classes = num_classes\n",
    "    model.head.classification_head.cls_logits = nn.Conv2d(\n",
    "        256, num_anchors * num_classes, kernel_size=3, padding=1\n",
    "    )\n",
    "\n",
    "    torch.nn.init.normal_(model.head.classification_head.cls_logits.weight, std=0.01)\n",
    "    torch.nn.init.constant_(model.head.classification_head.cls_logits.bias, -4.0)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_one_epoch_retinanet(model, loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, (images, targets) in enumerate(loader):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [\n",
    "            {\n",
    "                \"boxes\": t[\"boxes\"].to(device),\n",
    "                \"labels\": t[\"labels\"].to(device),\n",
    "            }\n",
    "            for t in targets\n",
    "        ]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                f\"[RetinaNet-Baseline][Epoch {epoch}] Step {step} \"\n",
    "                f\"loss: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "    print(f\"{timestamp()} — RetinaNet Baseline Epoch {epoch} Avg Loss: {total_loss / len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2860f33",
   "metadata": {},
   "source": [
    "### 5.1 Baseline inference helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2736826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:11:59.099579Z",
     "start_time": "2025-12-10T06:11:59.096247Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_inference_retinanet(model_path, image_path, config, save_output=False):\n",
    "    device = config.device\n",
    "    print(f\"Loading RetinaNet baseline: {model_path}\")\n",
    "\n",
    "    model = create_retinanet(config.num_classes_retina)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    if img_bgr is None:\n",
    "        raise ValueError(\"Image not found: \" + image_path)\n",
    "\n",
    "    rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    tensor = F.to_tensor(rgb).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model([tensor])[0]\n",
    "\n",
    "    vis = img_bgr.copy()\n",
    "    for box, score in zip(out[\"boxes\"], out[\"scores\"]):\n",
    "        if score < 0.4:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, box.tolist())\n",
    "        cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(vis, f\"{score:.2f}\", (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    if save_output:\n",
    "        ensure_dir(\"./output/inference_retina_baseline\")\n",
    "        out_path = \"./output/inference_retina_baseline/\" + Path(image_path).stem + \"_retina_pred.jpg\"\n",
    "        cv2.imwrite(out_path, vis)\n",
    "        print(\"Saved:\", out_path)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def batch_inference_retinanet(model_path, folder, config,\n",
    "                              save_dir=\"./output/inference_retina_baseline\"):\n",
    "    device = config.device\n",
    "    print(\"\\n=== Batch Inference: RetinaNet Baseline ===\")\n",
    "    ensure_dir(save_dir)\n",
    "\n",
    "    model = create_retinanet(config.num_classes_retina)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    image_files = [f for f in os.listdir(folder)\n",
    "                   if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for file in tqdm(image_files, desc=\"RetinaNet Baseline Batch Infer\"):\n",
    "            path_img = os.path.join(folder, file)\n",
    "            img_bgr = cv2.imread(path_img)\n",
    "            if img_bgr is None:\n",
    "                print(\"Skipping unreadable image:\", file)\n",
    "                continue\n",
    "\n",
    "            rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "            tensor = F.to_tensor(rgb).to(device)\n",
    "\n",
    "            out = model([tensor])[0]\n",
    "            vis = img_bgr.copy()\n",
    "\n",
    "            for box, score in zip(out[\"boxes\"], out[\"scores\"]):\n",
    "                if score < 0.4:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box.tolist())\n",
    "                cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(vis, f\"{score:.2f}\", (x1, y1 - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            save_path = os.path.join(save_dir, f\"{Path(file).stem}_retina_pred.jpg\")\n",
    "            cv2.imwrite(save_path, vis)\n",
    "\n",
    "    print(\"\\n=== RetinaNet Baseline Batch Inference Completed ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5de6d8d",
   "metadata": {},
   "source": [
    "## 8. Local focal loss & anchor utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "072bd2a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:11:59.106927Z",
     "start_time": "2025-12-10T06:11:59.101602Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# LOCAL CORRELATION-AWARE FOCAL LOSS\n",
    "# ======================================================================================\n",
    "\n",
    "class LocalFocalLoss2d(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, kernel_size=5, lambda_local=1.0, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.kernel_size = kernel_size\n",
    "        self.lambda_local = lambda_local\n",
    "        self.reduction = reduction\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size, stride=1, padding=kernel_size // 2)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        logits = logits.float()\n",
    "        targets = targets.float()\n",
    "\n",
    "        prob = torch.sigmoid(logits)\n",
    "        p_t = prob * targets + (1 - prob) * (1 - targets)\n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "\n",
    "        eps = 1e-8\n",
    "        focal_weight = alpha_t * (1 - p_t).pow(self.gamma)\n",
    "        base_loss = -focal_weight * torch.log(p_t.clamp(min=eps))\n",
    "\n",
    "        base_loss_spatial = base_loss.mean(dim=1, keepdim=True)\n",
    "        local_hardness = self.avg_pool(base_loss_spatial)\n",
    "\n",
    "        global_mean = local_hardness.mean().detach()\n",
    "        h_norm = local_hardness / (global_mean + eps)\n",
    "\n",
    "        local_weight = 1 + self.lambda_local * (h_norm - 1)\n",
    "        local_weight = torch.clamp(local_weight, min=0.1, max=3.0)\n",
    "        local_weight = local_weight.expand_as(base_loss)\n",
    "\n",
    "        final_loss = local_weight * base_loss\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            return final_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return final_loss.sum()\n",
    "        return final_loss\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# BOX ENCODING / NMS / ANCHORS\n",
    "# ======================================================================================\n",
    "\n",
    "def encode_boxes(anchors, gt_boxes):\n",
    "    ax = (anchors[:, 0] + anchors[:, 2]) / 2\n",
    "    ay = (anchors[:, 1] + anchors[:, 3]) / 2\n",
    "    aw = (anchors[:, 2] - anchors[:, 0])\n",
    "    ah = (anchors[:, 3] - anchors[:, 1])\n",
    "\n",
    "    gx = (gt_boxes[:, 0] + gt_boxes[:, 2]) / 2\n",
    "    gy = (gt_boxes[:, 1] + gt_boxes[:, 3]) / 2\n",
    "    gw = (gt_boxes[:, 2] - gt_boxes[:, 0])\n",
    "    gh = (gt_boxes[:, 3] - gt_boxes[:, 1])\n",
    "\n",
    "    tx = (gx - ax) / aw\n",
    "    ty = (gy - ay) / ah\n",
    "    tw = torch.log(gw / aw)\n",
    "    th = torch.log(gh / ah)\n",
    "    return torch.stack([tx, ty, tw, th], dim=1)\n",
    "\n",
    "\n",
    "def decode_boxes(anchors, deltas):\n",
    "    ax = (anchors[:, 0] + anchors[:, 2]) / 2\n",
    "    ay = (anchors[:, 1] + anchors[:, 3]) / 2\n",
    "    aw = (anchors[:, 2] - anchors[:, 0])\n",
    "    ah = (anchors[:, 3] - anchors[:, 1])\n",
    "\n",
    "    tx, ty, tw, th = deltas.unbind(dim=1)\n",
    "\n",
    "    gx = tx * aw + ax\n",
    "    gy = ty * ah + ay\n",
    "    gw = aw * torch.exp(tw)\n",
    "    gh = ah * torch.exp(th)\n",
    "\n",
    "    x1 = gx - gw / 2\n",
    "    y1 = gy - gh / 2\n",
    "    x2 = gx + gw / 2\n",
    "    y2 = gy + gh / 2\n",
    "    return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "\n",
    "def assign_anchors_to_gt(anchors, gt_boxes, iou_pos_thresh=0.5, iou_neg_thresh=0.4):\n",
    "    A = anchors.size(0)\n",
    "    device = anchors.device\n",
    "\n",
    "    labels = torch.full((A,), -1, dtype=torch.int64, device=device)\n",
    "    matched_gt_boxes = torch.zeros((A, 4), dtype=torch.float32, device=device)\n",
    "\n",
    "    if gt_boxes.numel() == 0:\n",
    "        labels[:] = 0\n",
    "        return labels, matched_gt_boxes\n",
    "\n",
    "    ious = box_iou(anchors, gt_boxes)\n",
    "    max_iou, max_idx = ious.max(dim=1)\n",
    "\n",
    "    labels[max_iou < iou_neg_thresh] = 0\n",
    "    labels[max_iou >= iou_pos_thresh] = 1\n",
    "\n",
    "    matched_gt_boxes[:] = gt_boxes[max_idx]\n",
    "    return labels, matched_gt_boxes\n",
    "\n",
    "\n",
    "def nms(boxes, scores, threshold=0.5):\n",
    "    return torchvision.ops.nms(boxes, scores, threshold)\n",
    "\n",
    "\n",
    "class MultiLevelAnchorGenerator:\n",
    "    \"\"\"Multi-scale anchors for FPN levels P3..P5.\"\"\"\n",
    "    def __init__(self, sizes_per_level, ratios, strides):\n",
    "        self.sizes_per_level = sizes_per_level\n",
    "        self.ratios = ratios\n",
    "        self.strides = strides\n",
    "\n",
    "    def _grid_anchors(self, grid_size, stride, sizes, device):\n",
    "        H, W = grid_size\n",
    "        shifts_x = (torch.arange(W, device=device) + 0.5) * stride\n",
    "        shifts_y = (torch.arange(H, device=device) + 0.5) * stride\n",
    "        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x, indexing=\"ij\")\n",
    "\n",
    "        shift_x = shift_x.reshape(-1)\n",
    "        shift_y = shift_y.reshape(-1)\n",
    "\n",
    "        anchor_boxes = []\n",
    "        for size in sizes:\n",
    "            for ratio in self.ratios:\n",
    "                size = float(size)\n",
    "                ratio = float(ratio)\n",
    "                w = size * math.sqrt(ratio)\n",
    "                h = size / math.sqrt(ratio)\n",
    "                anchors = torch.stack([\n",
    "                    shift_x - w / 2,\n",
    "                    shift_y - h / 2,\n",
    "                    shift_x + w / 2,\n",
    "                    shift_y + h / 2\n",
    "                ], dim=1)\n",
    "                anchor_boxes.append(anchors)\n",
    "\n",
    "        return torch.cat(anchor_boxes, dim=0)\n",
    "\n",
    "    def __call__(self, feature_shapes, device):\n",
    "        anchors_per_level = []\n",
    "        for (H, W), stride, sizes in zip(feature_shapes, self.strides, self.sizes_per_level):\n",
    "            anchors = self._grid_anchors((H, W), stride, sizes, device)\n",
    "            anchors_per_level.append(anchors)\n",
    "        return anchors_per_level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabfc0f4",
   "metadata": {},
   "source": [
    "## 9. LocalRetinaNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a882c271",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:11:59.118396Z",
     "start_time": "2025-12-10T06:11:59.109139Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# BACKBONE + FPN FOR LOCAL RETINANET (P3–P5 ONLY)\n",
    "# ======================================================================================\n",
    "\n",
    "class ResNetFPN(nn.Module):\n",
    "    \"\"\"ResNet50 backbone with FPN producing P3, P4, P5.\"\"\"\n",
    "    def __init__(self, backbone_name=\"resnet50\"):\n",
    "        super().__init__()\n",
    "        if backbone_name == \"resnet50\":\n",
    "            backbone = torchvision.models.resnet50(weights=\"DEFAULT\")\n",
    "            c3_channels = 512\n",
    "            c4_channels = 1024\n",
    "            c5_channels = 2048\n",
    "        else:\n",
    "            backbone = torchvision.models.resnet18(weights=None)\n",
    "            c3_channels = 128\n",
    "            c4_channels = 256\n",
    "            c5_channels = 512\n",
    "\n",
    "        self.conv1 = backbone.conv1\n",
    "        self.bn1 = backbone.bn1\n",
    "        self.relu = backbone.relu\n",
    "        self.maxpool = backbone.maxpool\n",
    "\n",
    "        self.layer1 = backbone.layer1  # C2\n",
    "        self.layer2 = backbone.layer2  # C3\n",
    "        self.layer3 = backbone.layer3  # C4\n",
    "        self.layer4 = backbone.layer4  # C5\n",
    "\n",
    "        self.lateral3 = nn.Conv2d(c3_channels, 256, 1)\n",
    "        self.lateral4 = nn.Conv2d(c4_channels, 256, 1)\n",
    "        self.lateral5 = nn.Conv2d(c5_channels, 256, 1)\n",
    "\n",
    "        self.p3_conv = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.p4_conv = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.p5_conv = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        c1 = self.maxpool(x)\n",
    "\n",
    "        c2 = self.layer1(c1)\n",
    "        c3 = self.layer2(c2)\n",
    "        c4 = self.layer3(c3)\n",
    "        c5 = self.layer4(c4)\n",
    "\n",
    "        p5 = self.lateral5(c5)\n",
    "        p4 = self.lateral4(c4) + Fnn.interpolate(p5, size=c4.shape[-2:], mode=\"nearest\")\n",
    "        p3 = self.lateral3(c3) + Fnn.interpolate(p4, size=c3.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "        p3 = self.p3_conv(p3)\n",
    "        p4 = self.p4_conv(p4)\n",
    "        p5 = self.p5_conv(p5)\n",
    "\n",
    "        return [p3, p4, p5]\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# RETINANET HEAD\n",
    "# ======================================================================================\n",
    "\n",
    "class RetinaNetHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_anchors, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        cls_layers = []\n",
    "        for _ in range(4):\n",
    "            cls_layers.append(nn.Conv2d(in_channels, in_channels, 3, padding=1))\n",
    "            cls_layers.append(nn.ReLU(inplace=True))\n",
    "        self.cls_subnet = nn.Sequential(*cls_layers)\n",
    "        self.cls_logits = nn.Conv2d(in_channels, num_anchors * num_classes, 3, padding=1)\n",
    "\n",
    "        box_layers = []\n",
    "        for _ in range(4):\n",
    "            box_layers.append(nn.Conv2d(in_channels, in_channels, 3, padding=1))\n",
    "            box_layers.append(nn.ReLU(inplace=True))\n",
    "        self.box_subnet = nn.Sequential(*box_layers)\n",
    "        self.bbox_pred = nn.Conv2d(in_channels, num_anchors * 4, 3, padding=1)\n",
    "\n",
    "        torch.nn.init.normal_(self.cls_logits.weight, std=0.01)\n",
    "        torch.nn.init.constant_(self.cls_logits.bias, -4.0)\n",
    "\n",
    "        torch.nn.init.normal_(self.bbox_pred.weight, std=0.01)\n",
    "        torch.nn.init.constant_(self.bbox_pred.bias, 0)\n",
    "\n",
    "    def forward(self, features):\n",
    "        cls_logits = []\n",
    "        bbox_regs = []\n",
    "        for f in features:\n",
    "            cls = self.cls_subnet(f)\n",
    "            cls_logits.append(self.cls_logits(cls))\n",
    "\n",
    "            box = self.box_subnet(f)\n",
    "            bbox_regs.append(self.bbox_pred(box))\n",
    "        return cls_logits, bbox_regs\n",
    "\n",
    "\n",
    "# ======================================================================================\n",
    "# LOCAL RETINANET\n",
    "# ======================================================================================\n",
    "\n",
    "class LocalRetinaNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, lambda_reg=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "        self.backbone = ResNetFPN(\"resnet50\")\n",
    "\n",
    "        self.num_anchors = 9\n",
    "        self.head = RetinaNetHead(256, self.num_anchors, num_classes)\n",
    "\n",
    "        base_sizes = [32, 64, 128]\n",
    "        scales = [1.0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]\n",
    "        sizes_per_level = [[b * s for s in scales] for b in base_sizes]\n",
    "        ratios = [0.5, 1.0, 2.0]\n",
    "        strides = [8, 16, 32]\n",
    "\n",
    "        self.anchor_gen = MultiLevelAnchorGenerator(sizes_per_level, ratios, strides)\n",
    "        self.cls_loss_fn = LocalFocalLoss2d(alpha=0.25, gamma=2.0, lambda_local=1.0)\n",
    "        self.reg_loss_fn = nn.SmoothL1Loss(reduction=\"sum\")\n",
    "\n",
    "    def _compute_loss_single_image(\n",
    "        self,\n",
    "        cls_logits_list,\n",
    "        bbox_regs_list,\n",
    "        anchors_per_level,\n",
    "        gt_boxes,\n",
    "        feature_shapes,\n",
    "        b,\n",
    "        device,\n",
    "    ):\n",
    "        num_levels = len(cls_logits_list)\n",
    "        level_offsets = []\n",
    "        logits_flat_by_level = []\n",
    "        box_flat_by_level = []\n",
    "\n",
    "        start = 0\n",
    "        for lvl in range(num_levels):\n",
    "            cls_l_b = cls_logits_list[lvl][b]\n",
    "            box_l_b = bbox_regs_list[lvl][b]\n",
    "            _, Hf, Wf = cls_l_b.shape\n",
    "            A = self.num_anchors\n",
    "\n",
    "            N_l = Hf * Wf * A\n",
    "\n",
    "            cls_flat = cls_l_b.permute(1, 2, 0).reshape(N_l, self.num_classes)\n",
    "            box_flat = box_l_b.permute(1, 2, 0).reshape(N_l, 4)\n",
    "\n",
    "            logits_flat_by_level.append(cls_flat)\n",
    "            box_flat_by_level.append(box_flat)\n",
    "\n",
    "            end = start + N_l\n",
    "            level_offsets.append((start, end, Hf, Wf))\n",
    "            start = end\n",
    "\n",
    "        logits_all = torch.cat(logits_flat_by_level, dim=0)\n",
    "        box_all = torch.cat(box_flat_by_level, dim=0)\n",
    "        anchors_all = torch.cat([a.to(device) for a in anchors_per_level], dim=0)\n",
    "\n",
    "        labels, matched_gt_boxes = assign_anchors_to_gt(anchors_all, gt_boxes)\n",
    "        fg_mask = labels == 1\n",
    "\n",
    "        cls_target_all = torch.zeros_like(logits_all)\n",
    "        if self.num_classes == 1:\n",
    "            cls_target_all[fg_mask, 0] = 1.0\n",
    "        else:\n",
    "            raise NotImplementedError(\"Current pipeline assumes num_classes=1.\")\n",
    "\n",
    "        cls_loss = torch.tensor(0.0, device=device)\n",
    "        for lvl in range(num_levels):\n",
    "            start, end, Hf, Wf = level_offsets[lvl]\n",
    "            targets_lvl_1d = cls_target_all[start:end]\n",
    "\n",
    "            t_map = targets_lvl_1d.reshape(\n",
    "                Hf, Wf, self.num_anchors * self.num_classes\n",
    "            ).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "            logits_lvl = cls_logits_list[lvl][b].unsqueeze(0)\n",
    "            cls_loss = cls_loss + self.cls_loss_fn(logits_lvl, t_map)\n",
    "\n",
    "        num_pos = int(fg_mask.sum().item())\n",
    "        if num_pos > 0:\n",
    "            pred_pos = box_all[fg_mask]\n",
    "            tgt_pos = encode_boxes(anchors_all[fg_mask], matched_gt_boxes[fg_mask])\n",
    "            reg_loss_sum = self.reg_loss_fn(pred_pos, tgt_pos)\n",
    "        else:\n",
    "            reg_loss_sum = torch.tensor(0.0, device=device)\n",
    "\n",
    "        return cls_loss, reg_loss_sum, num_pos\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        device = images[0].device\n",
    "\n",
    "        if not self.training or targets is None:\n",
    "            return self._forward_inference(images)\n",
    "\n",
    "        x = pad_images_to_batch(images).to(device)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        features = self.backbone(x)\n",
    "        cls_logits_list, bbox_regs_list = self.head(features)\n",
    "\n",
    "        feature_shapes = [(f.shape[2], f.shape[3]) for f in features]\n",
    "        anchors_per_level = self.anchor_gen(feature_shapes, device)\n",
    "\n",
    "        total_cls_loss = torch.tensor(0.0, device=device)\n",
    "        total_reg_sum = torch.tensor(0.0, device=device)\n",
    "        total_pos = 0\n",
    "\n",
    "        for b in range(B):\n",
    "            gt_boxes = targets[b][\"boxes\"].to(device)\n",
    "            cls_loss_b, reg_sum_b, num_pos_b = self._compute_loss_single_image(\n",
    "                cls_logits_list,\n",
    "                bbox_regs_list,\n",
    "                anchors_per_level,\n",
    "                gt_boxes,\n",
    "                feature_shapes,\n",
    "                b,\n",
    "                device,\n",
    "            )\n",
    "            total_cls_loss = total_cls_loss + cls_loss_b\n",
    "            total_reg_sum = total_reg_sum + reg_sum_b\n",
    "            total_pos += num_pos_b\n",
    "\n",
    "        if total_pos > 0:\n",
    "            avg_reg_loss = total_reg_sum / float(total_pos)\n",
    "        else:\n",
    "            avg_reg_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        total_loss = total_cls_loss + self.lambda_reg * avg_reg_loss\n",
    "\n",
    "        return {\n",
    "            \"loss_cls\": total_cls_loss,\n",
    "            \"loss_reg\": avg_reg_loss,\n",
    "            \"loss_total\": total_loss,\n",
    "        }\n",
    "\n",
    "    def _forward_inference(self, images):\n",
    "        device = images[0].device\n",
    "        x = pad_images_to_batch(images).to(device)\n",
    "        B = x.shape[0]\n",
    "\n",
    "        features = self.backbone(x)\n",
    "        cls_logits_list, bbox_regs_list = self.head(features)\n",
    "        feature_shapes = [(f.shape[2], f.shape[3]) for f in features]\n",
    "        anchors_per_level = self.anchor_gen(feature_shapes, device)\n",
    "\n",
    "        outputs = []\n",
    "        orig_sizes = [(img.shape[1], img.shape[2]) for img in images]\n",
    "\n",
    "        for b in range(B):\n",
    "            all_boxes = []\n",
    "            all_scores = []\n",
    "\n",
    "            for cls_l, box_l, anchors_l in zip(cls_logits_list, bbox_regs_list, anchors_per_level):\n",
    "                cls_l_img = cls_l[b]\n",
    "                box_l_img = box_l[b]\n",
    "\n",
    "                A_C, Hf, Wf = cls_l_img.shape\n",
    "                A = self.num_anchors\n",
    "                N_l = Hf * Wf * A\n",
    "\n",
    "                cls_flat = cls_l_img.permute(1, 2, 0).reshape(N_l, self.num_classes)\n",
    "                box_flat = box_l_img.permute(1, 2, 0).reshape(N_l, 4)\n",
    "\n",
    "                scores = torch.sigmoid(cls_flat[:, 0])\n",
    "                keep = scores > 0.05\n",
    "                if keep.sum() == 0:\n",
    "                    continue\n",
    "\n",
    "                scores_k = scores[keep]\n",
    "                box_k = box_flat[keep]\n",
    "                anchors_k = anchors_l[keep]\n",
    "\n",
    "                decoded = decode_boxes(anchors_k, box_k)\n",
    "                all_boxes.append(decoded)\n",
    "                all_scores.append(scores_k)\n",
    "\n",
    "            if len(all_boxes) == 0:\n",
    "                outputs.append({\n",
    "                    \"boxes\": torch.zeros((0, 4), device=device),\n",
    "                    \"scores\": torch.zeros((0,), device=device),\n",
    "                    \"labels\": torch.zeros((0,), dtype=torch.int64, device=device),\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            boxes_cat = torch.cat(all_boxes, dim=0)\n",
    "            scores_cat = torch.cat(all_scores, dim=0)\n",
    "\n",
    "            h, w = orig_sizes[b]\n",
    "            boxes_cat[:, 0] = boxes_cat[:, 0].clamp(min=0, max=w - 1)\n",
    "            boxes_cat[:, 2] = boxes_cat[:, 2].clamp(min=0, max=w - 1)\n",
    "            boxes_cat[:, 1] = boxes_cat[:, 1].clamp(min=0, max=h - 1)\n",
    "            boxes_cat[:, 3] = boxes_cat[:, 3].clamp(min=0, max=h - 1)\n",
    "\n",
    "            keep_nms = nms(boxes_cat, scores_cat, 0.5)\n",
    "            boxes_cat = boxes_cat[keep_nms]\n",
    "            scores_cat = scores_cat[keep_nms]\n",
    "            labels_cat = torch.ones_like(scores_cat, dtype=torch.int64, device=device)\n",
    "\n",
    "            outputs.append({\n",
    "                \"boxes\": boxes_cat,\n",
    "                \"scores\": scores_cat,\n",
    "                \"labels\": labels_cat,\n",
    "            })\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dfb146",
   "metadata": {},
   "source": [
    "## 10. Training loop for LocalRetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2625618b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:11:59.122273Z",
     "start_time": "2025-12-10T06:11:59.120358Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# TRAINING LOOP (LOCAL RETINANET)\n",
    "# ======================================================================================\n",
    "\n",
    "def train_one_epoch_local_retina(model, loader, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, (images, targets) in enumerate(loader):\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = loss_dict[\"loss_total\"]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if step % 10 == 0:\n",
    "            print(\n",
    "                f\"[Local-RetinaNet][Epoch {epoch}] Step {step} \"\n",
    "                f\"cls: {loss_dict['loss_cls'].item():.4f}, \"\n",
    "                f\"reg: {loss_dict['loss_reg'].item():.4f}, \"\n",
    "                f\"total: {loss.item():.4f}\"\n",
    "            )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"{timestamp()} — Local RetinaNet Epoch {epoch} Avg Loss: {total_loss / len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d23b759",
   "metadata": {},
   "source": [
    "## 11. COCO evaluation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90029a30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:11:59.126275Z",
     "start_time": "2025-12-10T06:11:59.123832Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# COCO EVAL (shared for both models)\n",
    "# ======================================================================================\n",
    "\n",
    "def convert_to_coco_predictions(outputs, image_ids):\n",
    "    results = []\n",
    "    for out, img_id in zip(outputs, image_ids):\n",
    "        for box, score, label in zip(out[\"boxes\"], out[\"scores\"], out[\"labels\"]):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            results.append({\n",
    "                \"image_id\": int(img_id),\n",
    "                \"category_id\": int(label),\n",
    "                \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
    "                \"score\": float(score),\n",
    "            })\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_coco(model, loader, ann_file, device, out_name):\n",
    "    fix_coco_json(ann_file)\n",
    "    print(f\"Evaluating COCO mAP → {out_name} ...\")\n",
    "    model.eval()\n",
    "    coco = COCO(ann_file)\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(loader, desc=\"COCO Eval\"):\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "            image_ids = [t[\"image_id\"].item() for t in targets]\n",
    "            results.extend(convert_to_coco_predictions(outputs, image_ids))\n",
    "\n",
    "    ensure_dir(\"./output/coco_eval\")\n",
    "    pred_file = f\"./output/coco_eval/{out_name}\"\n",
    "    with open(pred_file, \"w\") as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    if len(results) == 0:\n",
    "        print(\"No predictions generated — skipping COCOeval (AP will be 0).\")\n",
    "        return\n",
    "\n",
    "    coco_dt = coco.loadRes(pred_file)\n",
    "    ev = COCOeval(coco, coco_dt, \"bbox\")\n",
    "    ev.evaluate()\n",
    "    ev.accumulate()\n",
    "    ev.summarize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f303b",
   "metadata": {},
   "source": [
    "## 12. LocalRetinaNet inference helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebcc56e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:11:59.131748Z",
     "start_time": "2025-12-10T06:11:59.127858Z"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================================================\n",
    "# LOCAL RETINANET INFERENCE HELPERS\n",
    "# ======================================================================================\n",
    "\n",
    "def run_single_inference_local(model_path, image_path, config):\n",
    "    device = config.device\n",
    "    print(f\"Loading Local RetinaNet: {model_path}\")\n",
    "\n",
    "    # Load model\n",
    "    model = LocalRetinaNet(\n",
    "        num_classes=config.num_classes_local,\n",
    "        lambda_reg=config.lambda_reg_local,\n",
    "    ).to(device)\n",
    "\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    img_bgr = cv2.imread(image_path)\n",
    "    if img_bgr is None:\n",
    "        raise ValueError(\"Image not found: \" + image_path)\n",
    "\n",
    "    orig_h, orig_w = img_bgr.shape[:2]\n",
    "\n",
    "    # Convert to PIL for ResizeForDetection\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    resize = ResizeForDetection(max_side=1024)\n",
    "\n",
    "    # Fake target (ResizeForDetection expects a dict)\n",
    "    fake_target = {\"boxes\": torch.zeros((0,4))}\n",
    "\n",
    "    resized_img_tensor, _ = resize(img_pil, fake_target)  # tensor CHW\n",
    "\n",
    "    resized_h, resized_w = resized_img_tensor.shape[1], resized_img_tensor.shape[2]\n",
    "\n",
    "    scale_x = orig_w / resized_w\n",
    "    scale_y = orig_h / resized_h\n",
    "\n",
    "    # Move to device\n",
    "    tensor = resized_img_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model([tensor])\n",
    "        out = outputs[0]\n",
    "\n",
    "    boxes_scaled = out[\"boxes\"].clone()\n",
    "    boxes_scaled[:, 0] *= scale_x   # x1\n",
    "    boxes_scaled[:, 2] *= scale_x   # x2\n",
    "    boxes_scaled[:, 1] *= scale_y   # y1\n",
    "    boxes_scaled[:, 3] *= scale_y   # y2\n",
    "\n",
    "    vis = img_bgr.copy()\n",
    "    for box, score in zip(boxes_scaled, out[\"scores\"]):\n",
    "        if score < 0.24:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, box.tolist())\n",
    "        cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(vis, f\"{score:.2f}\", (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def batch_inference_local(model_path, folder, config,\n",
    "                          save_dir=\"./output/inference_retina_local\"):\n",
    "    device = config.device\n",
    "    print(\"\\n=== Batch Inference: Local RetinaNet ===\")\n",
    "    ensure_dir(save_dir)\n",
    "\n",
    "    model = LocalRetinaNet(\n",
    "        num_classes=config.num_classes_local,\n",
    "        lambda_reg=config.lambda_reg_local,\n",
    "    ).to(device)\n",
    "\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    image_files = [f for f in os.listdir(folder)\n",
    "                   if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for file in tqdm(image_files, desc=\"Local RetinaNet Batch Infer\"):\n",
    "            path_img = os.path.join(folder, file)\n",
    "            img_bgr = cv2.imread(path_img)\n",
    "            if img_bgr is None:\n",
    "                print(\"Skipping unreadable image:\", file)\n",
    "                continue\n",
    "\n",
    "            rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "            tensor = F.to_tensor(rgb).to(device)\n",
    "\n",
    "            outputs = model([tensor])\n",
    "            out_det = outputs[0]\n",
    "\n",
    "            vis = img_bgr.copy()\n",
    "            for box, score in zip(out_det[\"boxes\"], out_det[\"scores\"]):\n",
    "                if score < 0.3:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = map(int, box.tolist())\n",
    "                cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(vis, f\"{score:.2f}\", (x1, y1 - 5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            save_path = os.path.join(save_dir, f\"{Path(file).stem}_retina_local.jpg\")\n",
    "            cv2.imwrite(save_path, vis)\n",
    "\n",
    "    print(\"\\n=== Local RetinaNet Batch Inference Completed ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce387a02",
   "metadata": {},
   "source": [
    "## 13. [Training] Train LocalRetinaNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9151f5a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:12:11.106020Z",
     "start_time": "2025-12-10T06:11:59.133438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Local-RetinaNet][Epoch 1] Step 0 cls: 0.0592, reg: 0.0824, total: 0.0675\n",
      "[Local-RetinaNet][Epoch 1] Step 10 cls: 0.0698, reg: 0.0820, total: 0.0781\n",
      "[Local-RetinaNet][Epoch 1] Step 20 cls: 0.0844, reg: 0.0782, total: 0.0922\n",
      "[Local-RetinaNet][Epoch 1] Step 30 cls: 0.0775, reg: 0.0863, total: 0.0862\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 29\u001B[39m\n\u001B[32m     26\u001B[39m optimizer_local = optim.Adam(local_model.parameters(), lr=config.lr)\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m ep \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m1\u001B[39m, config.num_epochs_local + \u001B[32m1\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m29\u001B[39m     \u001B[43mtrain_one_epoch_local_retina\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocal_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader_local\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_local\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m torch.save(local_model.state_dict(), config.save_local_model_path)\n\u001B[32m     32\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mSaved Local RetinaNet model:\u001B[39m\u001B[33m\"\u001B[39m, config.save_local_model_path)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 17\u001B[39m, in \u001B[36mtrain_one_epoch_local_retina\u001B[39m\u001B[34m(model, loader, optimizer, device, epoch)\u001B[39m\n\u001B[32m     15\u001B[39m optimizer.zero_grad()\n\u001B[32m     16\u001B[39m loss.backward()\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m total_loss += loss.item()\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m step % \u001B[32m10\u001B[39m == \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/myenv/lib/python3.14/site-packages/torch/optim/optimizer.py:517\u001B[39m, in \u001B[36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    512\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    513\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    514\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    515\u001B[39m             )\n\u001B[32m--> \u001B[39m\u001B[32m517\u001B[39m out = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    518\u001B[39m \u001B[38;5;28mself\u001B[39m._optimizer_step_code()\n\u001B[32m    520\u001B[39m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/myenv/lib/python3.14/site-packages/torch/optim/optimizer.py:82\u001B[39m, in \u001B[36m_use_grad_for_differentiable.<locals>._use_grad\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     80\u001B[39m     torch.set_grad_enabled(\u001B[38;5;28mself\u001B[39m.defaults[\u001B[33m\"\u001B[39m\u001B[33mdifferentiable\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     81\u001B[39m     torch._dynamo.graph_break()\n\u001B[32m---> \u001B[39m\u001B[32m82\u001B[39m     ret = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     83\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m     84\u001B[39m     torch._dynamo.graph_break()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/myenv/lib/python3.14/site-packages/torch/optim/adam.py:247\u001B[39m, in \u001B[36mAdam.step\u001B[39m\u001B[34m(self, closure)\u001B[39m\n\u001B[32m    235\u001B[39m     beta1, beta2 = group[\u001B[33m\"\u001B[39m\u001B[33mbetas\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    237\u001B[39m     has_complex = \u001B[38;5;28mself\u001B[39m._init_group(\n\u001B[32m    238\u001B[39m         group,\n\u001B[32m    239\u001B[39m         params_with_grad,\n\u001B[32m   (...)\u001B[39m\u001B[32m    244\u001B[39m         state_steps,\n\u001B[32m    245\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m247\u001B[39m     \u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    248\u001B[39m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    249\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    250\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    251\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    253\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mamsgrad\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    255\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    256\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    257\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    258\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    259\u001B[39m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mweight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    260\u001B[39m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43meps\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    261\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    262\u001B[39m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mforeach\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcapturable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    264\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdifferentiable\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    265\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfused\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    266\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mgrad_scale\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    267\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfound_inf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    268\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mdecoupled_weight_decay\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    269\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    271\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/myenv/lib/python3.14/site-packages/torch/optim/optimizer.py:150\u001B[39m, in \u001B[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m disabled_func(*args, **kwargs)\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/myenv/lib/python3.14/site-packages/torch/optim/adam.py:953\u001B[39m, in \u001B[36madam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[39m\n\u001B[32m    950\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    951\u001B[39m     func = _single_tensor_adam\n\u001B[32m--> \u001B[39m\u001B[32m953\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    954\u001B[39m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    955\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    956\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    957\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    958\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    959\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m=\u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    961\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    962\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    963\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    964\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    965\u001B[39m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[43m=\u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    968\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    969\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    970\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    971\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdecoupled_weight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/myenv/lib/python3.14/site-packages/torch/optim/adam.py:466\u001B[39m, in \u001B[36m_single_tensor_adam\u001B[39m\u001B[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001B[39m\n\u001B[32m    462\u001B[39m         exp_avg_sq.mul_(beta2).addcmul_(\n\u001B[32m    463\u001B[39m             grad, grad, value=cast(\u001B[38;5;28mfloat\u001B[39m, \u001B[32m1\u001B[39m - beta2)\n\u001B[32m    464\u001B[39m         )\n\u001B[32m    465\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m     \u001B[43mexp_avg_sq\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43maddcmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[32m    468\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n\u001B[32m    469\u001B[39m     step = step_t\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Example: Train LocalRetinaNet\n",
    "# ============================\n",
    "\n",
    "device = config.device\n",
    "ensure_dir(\"./output/pt-models\")\n",
    "\n",
    "train_ds_local = SKU110K_COCO(\n",
    "    config.train_images,\n",
    "    config.train_annotations,\n",
    "    transforms=ResizeForDetection(max_side=1024),\n",
    ")\n",
    "train_loader_local = DataLoader(\n",
    "    train_ds_local,\n",
    "    batch_size=config.batch_size_local,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "local_model = LocalRetinaNet(\n",
    "    num_classes=config.num_classes_local,\n",
    "    lambda_reg=config.lambda_reg_local,\n",
    ").to(device)\n",
    "\n",
    "optimizer_local = optim.Adam(local_model.parameters(), lr=config.lr)\n",
    "\n",
    "for ep in range(1, config.num_epochs_local + 1):\n",
    "    train_one_epoch_local_retina(local_model, train_loader_local, optimizer_local, device, ep)\n",
    "\n",
    "torch.save(local_model.state_dict(), config.save_local_model_path)\n",
    "print(\"Saved Local RetinaNet model:\", config.save_local_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef61cbd2",
   "metadata": {},
   "source": [
    "## 14. [Training] Train RetinaNet baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54deb157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Example: Train torchvision RetinaNet baseline\n",
    "# ============================\n",
    "\n",
    "device = config.device\n",
    "ensure_dir(\"./output/pt-models\")\n",
    "\n",
    "train_ds_retina = SKU110K_COCO(\n",
    "    config.train_images,\n",
    "    config.train_annotations,\n",
    "    transforms=None,\n",
    ")\n",
    "train_loader_retina = DataLoader(\n",
    "    train_ds_retina,\n",
    "    batch_size=config.batch_size_retina,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "retina_model = create_retinanet(config.num_classes_retina).to(device)\n",
    "optimizer_retina = optim.Adam(retina_model.parameters(), lr=config.lr)\n",
    "\n",
    "for ep in range(1, config.num_epochs_retina + 1):\n",
    "    train_one_epoch_retinanet(retina_model, train_loader_retina, optimizer_retina, device, ep)\n",
    "\n",
    "torch.save(retina_model.state_dict(), config.save_retinanet_model_path)\n",
    "print(\"Saved RetinaNet baseline:\", config.save_retinanet_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9768c674",
   "metadata": {},
   "source": [
    "## 15. [Evaluation] COCO evaluation (LocalRetinaNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1af563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:16:23.580004Z",
     "start_time": "2025-12-10T06:12:24.559929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating COCO mAP → pred_retinanet_local.json ...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COCO Eval: 100%|██████████| 50/50 [03:49<00:00,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=1.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=4.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# COCO Evaluation (LocalRetinaNet)\n",
    "# ============================\n",
    "\n",
    "device = config.device\n",
    "\n",
    "test_ds_local = SKU110K_COCO(\n",
    "    config.test_images,\n",
    "    config.test_annotations,\n",
    "    transforms=ResizeForDetection(max_side=1024),\n",
    ")\n",
    "test_loader_local = DataLoader(\n",
    "    test_ds_local,\n",
    "    batch_size=config.batch_size_local,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "local_model_eval = LocalRetinaNet(\n",
    "    num_classes=config.num_classes_local,\n",
    "    lambda_reg=config.lambda_reg_local,\n",
    ").to(device)\n",
    "local_model_eval.load_state_dict(torch.load(config.save_local_model_path, map_location=device))\n",
    "\n",
    "evaluate_coco(local_model_eval, test_loader_local, config.test_annotations, device,\n",
    "              out_name=\"pred_retinanet_local.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51895ac",
   "metadata": {},
   "source": [
    "## 16. [Evaluation] COCO evaluation (RetinaNet baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83b7ffa398b1390c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T06:27:48.465234Z",
     "start_time": "2025-12-10T06:27:33.056140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loading baseline checkpoint: ./output/pt-models/retinanet_sku110k.pth\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COCO Eval (Baseline): 100%|██████████| 25/25 [00:10<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions: ./output/coco_eval/pred_retinanet_baseline.json\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=3.92s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.377\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.608\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.430\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.428\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.050\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.428\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.428\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = -1.000\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# COCO Evaluation (RetinaNet baseline)\n",
    "# ================================================================\n",
    "\n",
    "test_ds = SKU110K_COCO(\n",
    "    root=config.test_images,\n",
    "    annotation_json=config.test_annotations,\n",
    "    transforms=None,   # Baseline RetinaNet expects raw image sizes\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=config.batch_size_retina,\n",
    "    shuffle=False,\n",
    "    num_workers=config.num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "device = config.device\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = create_retinanet(config.num_classes_retina).to(device)\n",
    "\n",
    "print(\"Loading baseline checkpoint:\", config.save_retinanet_model_path)\n",
    "state = torch.load(config.save_retinanet_model_path, map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "fix_coco_json(config.test_annotations)\n",
    "coco = COCO(config.test_annotations)\n",
    "\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(test_loader, desc=\"COCO Eval (Baseline)\"):\n",
    "        images = [img.to(device) for img in images]\n",
    "\n",
    "        outputs = model(images)\n",
    "        img_ids = [t[\"image_id\"].item() for t in targets]\n",
    "\n",
    "        results.extend(convert_to_coco_predictions(outputs, img_ids))\n",
    "\n",
    "ensure_dir(\"./output/coco_eval\")\n",
    "pred_file = \"./output/coco_eval/pred_retinanet_baseline.json\"\n",
    "\n",
    "with open(pred_file, \"w\") as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print(\"Saved predictions:\", pred_file)\n",
    "\n",
    "if len(results) == 0:\n",
    "    print(\"No predictions were produced — COCOeval skipped.\")\n",
    "else:\n",
    "    coco_dt = coco.loadRes(pred_file)\n",
    "    evaluator = COCOeval(coco, coco_dt, \"bbox\")\n",
    "    evaluator.evaluate()\n",
    "    evaluator.accumulate()\n",
    "    evaluator.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a7951",
   "metadata": {},
   "source": [
    "## 17. [Inference] Single-image inference (LocalRetinaNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_single_inference_local(\n",
    "    config.save_local_model_path,\n",
    "    config.infer_image_path,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afe1d52",
   "metadata": {},
   "source": [
    "## 18. [Inference] Single-image inference (LocalRetina baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157edcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference_retinanet(\n",
    "    config.save_retinanet_model_path,\n",
    "    config.infer_image_path,\n",
    "    config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
